# -*- coding: utf-8 -*-
"""Q&A_Evaluacion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LLzoythUWc_igSg7WQdHuOFCpf8WQMDp

# Rag Evaluation & RAGas
"""

from google.colab import drive
drive.mount('/content/drive')

"""Este notebook contiene lo siguiente:
- Carga de datos en Langchain, con la base de datos chroma que contiene los documentos procesados en la fase anterior.
- Creaci√≥n de Q&A con una LLM afinada especialmente para tareas de RAG.
- üß™ Creaci√≥n de Datos Sint√©ticos: Entender el proceso y la importancia de generar datos sint√©ticos para la evaluaci√≥n RAG.
- üõ†Ô∏è Uso de Ragas para una evaluaci√≥n completa del rendimiento del modelo RAG a trav√©s de varias m√©tricas.


"""

!pip install -qU langchain openai ragas arxiv pymupdf chromadb tiktoken accelerate bitsandbytes datasets sentence_transformers FlagEmbedding ninja  tqdm rank_bm25 transformers

!pip install -U flash_attn --no-build-isolation

"""## Carga de datos en Langchain

"""

!pip install langchain pypdf pytube youtube-transcript-api openai langchain_experimental langchain_openai

"""Se crea la Base de Datos Vectorial con los documentos que se procesaron en la fase anterior.

Se usar√° [ChromaDB](https://www.trychroma.com/) como base de datos y [hiiamsid/sentence_similarity_spanish_es] como modelo de embedding.

"""

from langchain.vectorstores import Chroma
import chromadb
import os
from sentence_transformers import SentenceTransformer

# --- CONFIGURACI√ìN ---
CHROMA_DB_PATH = "/content/drive/MyDrive/Colab Notebooks/Proyecto Final/07_CHROMADB/"  # Ruta donde est√° almacenada la base de datos
COLLECTION_NAME = "documentos_legales"

# --- CARGAR EL MISMO MODELO DE EMBEDDINGS QUE SE US√ì EN CHROMA ---
# --- MODELO DE EMBEDDINGS (Debe coincidir con el usado para indexar) ---
embedding_model = SentenceTransformer("hiiamsid/sentence_similarity_spanish_es")


# --- DEFINIR UNA FUNCI√ìN COMPATIBLE PARA LANGCHAIN ---


class CustomEmbeddingFunction:
    def __init__(self, model):
        self.model = model
    def embed_documents(self, texts):
        """Convierte una lista de textos en una lista de listas de floats"""
        return self.model.encode(texts, convert_to_numpy=True).tolist()

    def embed_query(self, text):
        """Convierte una consulta en una lista de floats"""
        return self.model.encode([text], convert_to_numpy=True).tolist()[0]  # ‚úÖ Elimina el nivel extra de listas

# Crear instancia compatible con LangChain
embedding_function = CustomEmbeddingFunction(embedding_model)

# --- CARGAR BASE DE DATOS CHROMA en LangChain---
vectorstore = Chroma(
    persist_directory=CHROMA_DB_PATH,  # Cargar desde el disco
    collection_name=COLLECTION_NAME,
    embedding_function=embedding_function  # ‚úÖ Usa la misma funci√≥n de embeddings
)


num_docs = vectorstore._collection.count()
print(f"üìä La colecci√≥n '{COLLECTION_NAME}' tiene {num_docs} documentos.")

# --- CREAR EL RETRIEVER ---
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})  # Recuperar los 5 documentos m√°s relevantes

# --- PROBAR EL RETRIEVER ---
query = "residuos qu√≠micos"
docs = retriever.get_relevant_documents(query)

# --- MOSTRAR RESULTADOS ---
if docs:
    print("üîπ Documentos Relevantes Recuperados:")
    for i, doc in enumerate(docs):
        print(f"\nüìÑ Documento {i+1}: {doc.page_content}")
else:
    print("‚ö†Ô∏è No se encontraron documentos relevantes.")

"""## Preguntas y Respuestas con un modelo preparado para RAG

Con el pipeline de RAG creado, y una LLM afinada especialmente para tareas de RAG, se cre√≥ un Q&A.

Se us√≥ el modelo [llmware/dragon-deci-7b-v0](https://huggingface.co/llmware/dragon-deci-7b-v0). Un modelo de la serie DRAGON afinado especialmente para tareas relacionadas con extracci√≥n de informaci√≥n a partir de un contexto.
"""

import os
from operator import itemgetter
import torch
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, pipeline
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
# --- CARGAR MODELO DECI_DRAGON ---
#Se realiz√≥ un guardado previo del modelo

model_path = os.path.abspath("/content/drive/MyDrive/Colab Notebooks/Proyecto Final/deci_dragon_model/")

model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

text_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=4096,
    temperature=1e-3,
    do_sample=True,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id
)

deci_dragon = HuggingFacePipeline(pipeline=text_pipeline)

#TEMPLATE
from langchain.prompts import ChatPromptTemplate
template = """<human>: Answer the question based only on the following context. Respond in Spanish. If you cannot answer the question with the context, please respond with 'I don't know':
### CONTEXT
{context}
### QUESTION
Question: {question}
\n
<bot>:
"""
prompt = ChatPromptTemplate.from_template(template)

#PIPELINE
retrieval_augmented_qa_chain = (
    {"context": itemgetter("question") | retriever, "question": itemgetter("question")}
    | RunnablePassthrough.assign(context=itemgetter("context"))
    | {"response": prompt | deci_dragon, "context": itemgetter("context")}
)

#TEST
question = "¬øC√≥mo se gestionan los residuos qu√≠micos?"

result = retrieval_augmented_qa_chain.invoke({"question" : question})

print(result['response'])

"""## Creaci√≥n de un dataset de evaluaci√≥n

Se puede evaluar el modelo en formato `batch` o en `realtime`. Para hacerlo en formato batch se necesita un dataset de preguntas y respuestas fiables para luego poder comparar el resultado de nuestro modelo con la respuesta esperada.

Este dataset lo podemos crear nosotros manualmente o... ¬°usar un modelo m√°s grande con altas capacidades para generarlo!

Usaremos GPT-3.5 para generar las preguntas y GPT-4 para contestarlas.

Finalmente, nuestro dataset deber√° tener:

- **Preguntas:** Estos son los prompts que el modelo RAG tratar√°.
- **Verdades Fundamentales:** Estas son las respuestas correctas a las preguntas. Las utilizar√°s como referencia para medir con qu√© precisi√≥n responde tu modelo RAG.
- **Respuestas Predichas:** Estas son las respuestas que genera el modelo RAG. La tarea clave es comparar estas respuestas con las verdades fundamentales para evaluar la precisi√≥n del modelo.
- **Contextos:** Estos proporcionan el antecedente o informaci√≥n suplementaria necesaria que tu modelo RAG utiliza para elaborar sus respuestas. Entender c√≥mo tu modelo aprovecha este contexto es vital para evaluar su eficacia al incorporar informaci√≥n externa en sus respuestas.

"""

import os
import getpass

os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")

from langchain.output_parsers import ResponseSchema
from langchain.output_parsers import StructuredOutputParser

question_schema = ResponseSchema(
    name="question",
    description="a question about the context."
)

question_response_schemas = [
    question_schema,
]

question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)

format_instructions = question_output_parser.get_format_instructions()

question_generation_llm = ChatOpenAI(model="gpt-4o-mini")

bare_prompt_template = "{content}"

bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)

from langchain.prompts import ChatPromptTemplate

qa_template = """\
  You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.
  question: a question about the context.
  Format the output as JSON with the following keys:
  question
  context: {context}
"""

prompt_template = ChatPromptTemplate.from_template(template=qa_template)

messages = prompt_template.format_messages(
    context=docs[0],
    format_instructions=format_instructions
)

question_generation_chain = bare_template | question_generation_llm

response = question_generation_chain.invoke({"content" : messages})

output_dict = question_output_parser.parse(response.content)

from langchain.prompts import ChatPromptTemplate

qa_template = """\
  You are an expert in Ecuadorian environmental legal regulations who creates frequently asked questions and answers for environmental professionals in Ecuador. For each context, create a question that is specific to the context. Avoid creating generic or general questions. Generate questions in Spanish
  question: a question about the context.
  Format the output as JSON with the following keys:
  question
  context: {context}
"""

prompt_template = ChatPromptTemplate.from_template(template=qa_template)

messages = prompt_template.format_messages(
    context=docs[0],
    format_instructions=format_instructions
)

question_generation_chain = bare_template | question_generation_llm

response = question_generation_chain.invoke({"content" : messages})

output_dict = question_output_parser.parse(response.content)

for k, v in output_dict.items():
  print(k)
  print(v)

from tqdm import tqdm
import random

random.seed(42)
qac_triples = []

loop = 5

for text in tqdm(random.sample(docs, loop)):

  messages = prompt_template.format_messages(
      context=text,
      format_instructions=format_instructions
  )

  response = question_generation_chain.invoke({"content" : messages})

  try:
    output_dict = question_output_parser.parse(response.content)
  except Exception as e:
    continue

  output_dict["context"] = text
  qac_triples.append(output_dict)

for qac in qac_triples:
  print(qac)

answer_generation_llm = ChatOpenAI(model="gpt-4o", temperature=0)

answer_schema = ResponseSchema(
    name="answer",
    description="an answer to the question"
)

answer_response_schemas = [
    answer_schema,
]

answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)
format_instructions = answer_output_parser.get_format_instructions()

qa_template = """\
  You are an expert in Ecuadorian environmental legal regulations who creates frequently asked questions and answers for environmental professionals in Ecuador. For each question and context, create an answer. Generate questions and answers in Spanish.
  answer: a answer about the context.
  Format the output as JSON with the following keys:
  answer
  question: {question}
  context: {context}
"""

prompt_template = ChatPromptTemplate.from_template(template=qa_template)
messages = prompt_template.format_messages(
    context=qac_triples[0]["context"],
    question=qac_triples[0]["question"],
    format_instructions=format_instructions
)

answer_generation_chain = bare_template | answer_generation_llm
response = answer_generation_chain.invoke({"content" : messages})
output_dict = answer_output_parser.parse(response.content)

for k, v in output_dict.items():
  print(k)
  print(v)

for triple in tqdm(qac_triples):

  messages = prompt_template.format_messages(
      context=triple["context"],
      question=triple["question"],
      format_instructions=format_instructions
  )

  response = answer_generation_chain.invoke({"content" : messages})

  try:
    output_dict = answer_output_parser.parse(response.content)
  except Exception as e:
    continue

  triple["answer"] = output_dict["answer"]

import pandas as pd
from datasets import Dataset

ground_truth_qac_set = pd.DataFrame(qac_triples)
ground_truth_qac_set["context"] = ground_truth_qac_set["context"].map(lambda x: str(x.page_content))
ground_truth_qac_set = ground_truth_qac_set.rename(columns={"answer" : "ground_truth"})

eval_dataset = Dataset.from_pandas(ground_truth_qac_set)

eval_dataset

eval_dataset[0]

from huggingface_hub import login
login()

eval_dataset.push_to_hub("taniagdn/ragas-eval-dataset-amb")

"""## RAG Evaluation using RAGas

Recordemos las m√©tricas que se eval√∫an:
- **Relevancia de la Respuesta:** La pertinencia de la respuesta del modelo RAG al prompt dado.
- **Fidelidad:** Si las respuestas son fieles a los hechos proporcionados en el contexto.
- **Precisi√≥n del Contexto:** La capacidad del modelo para clasificar la informaci√≥n relevante al principio.
- **Correcci√≥n de la Respuesta:** La exactitud de la respuesta en comparaci√≥n con la verdad objetiva.
"""

from ragas.metrics import (
    answer_relevancy,
    faithfulness#,
    #context_recall,
    #context_precision,
    #answer_correctness,
    #answer_similarity
)
from ragas import evaluate

def create_ragas_dataset(rag_pipeline, eval_dataset):

  rag_dataset = []

  for row in tqdm(eval_dataset):
    answer = rag_pipeline.invoke({"question" : row["question"]})
    rag_dataset.append(
        {"question" : row["question"],
         "answer" : answer["response"],
         "contexts" : [context.page_content for context in answer["context"]],
         "ground_truths" : [row["ground_truth"]]
         }
    )

  rag_df = pd.DataFrame(rag_dataset)
  rag_eval_dataset = Dataset.from_pandas(rag_df)

  return rag_eval_dataset

def evaluate_ragas_dataset(ragas_dataset):

  result = evaluate(
    ragas_dataset,
    metrics=[
        #context_precision,
        faithfulness,
        answer_relevancy#,
        #context_recall,
        #answer_correctness,
        #answer_similarity
    ],
  )

  return result

"""Ahora ya podemos evaluar el dataset. Si quisi√©ramos evaluar solo un registro, lo har√≠amos con un solo registro ¬øno?

"""

from tqdm import tqdm
import pandas as pd
basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)

basic_qa_result = evaluate_ragas_dataset(basic_qa_ragas_dataset)

"""## Resultados gr√°ficos

"""

import matplotlib.pyplot as plt
def plot_metrics_with_values(metrics_dict, title='RAG Metrics'):
    """
    Plots a bar chart for metrics contained in a dictionary and annotates the values on the bars.
    Args:
    metrics_dict (dict): A dictionary with metric names as keys and values as metric scores.
    title (str): The title of the plot.
    """
    names = list(metrics_dict.keys())
    values = list(metrics_dict.values())
    plt.figure(figsize=(10, 6))
    bars = plt.barh(names, values, color='skyblue')
    # Adding the values on top of the bars
    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.01,  # x-position
                 bar.get_y() + bar.get_height() / 2,  # y-position
                 f'{width:.4f}',  # value
                 va='center')
    plt.xlabel('Score')
    plt.title(title)
    plt.xlim(0, 1)  # Setting the x-axis limit to be from 0 to 1
    plt.show()

basic_qa_result

plot_metrics_with_values(basic_qa_result, "Base Retriever ragas Metrics")

"""- **Context Precision:** Esta m√©trica eval√∫a qu√© tan bien el sistema puede seleccionar informaci√≥n relevante del contexto proporcionado. Un valor alto indica que el sistema es capaz de distinguir y priorizar la informaci√≥n m√°s relevante para la consulta.

- **Faithfulness:** Mide la fidelidad de las respuestas generadas respecto al contexto original. Una puntuaci√≥n alta significa que la mayor√≠a de la informaci√≥n presente en las respuestas puede ser rastreada de manera fiable al contexto, garantizando que las respuestas son factualmente consistentes.

- **Answer Relevancy:** Esta m√©trica determina qu√© tan relevantes son las respuestas a las preguntas formuladas. Valores altos indican que el sistema entiende bien la consulta y proporciona respuestas que se ajustan estrechamente a la necesidad de informaci√≥n del usuario.

- **Context Recall:** Eval√∫a la capacidad del sistema para recuperar toda la informaci√≥n relevante disponible en el contexto o base de datos para una consulta espec√≠fica. Una puntuaci√≥n alta aqu√≠ sugerir√≠a que el sistema es muy eficiente en encontrar y utilizar toda la informaci√≥n pertinente.

- **Context Relevancy:** Esta m√©trica examina si el contexto recuperado y utilizado por el sistema para responder a una consulta es realmente pertinente para la pregunta hecha. Un valor bajo podr√≠a indicar que el sistema est√° recuperando mucha informaci√≥n que, aunque es relevante para el contexto en general, no es √∫til para la consulta espec√≠fica.

- **Answer Correctness:** Mide la precisi√≥n o correcci√≥n de las respuestas dadas. Un valor moderadamente alto indica que una buena parte de las respuestas son correctas, pero tambi√©n hay espacio para la mejora en la precisi√≥n de las respuestas.

- **Answer Similarity:** Esta m√©trica compara las respuestas generadas con las respuestas esperadas o ideales para ver qu√© tan cercanas son en t√©rminos de contenido y contexto. Un valor alto indica que las respuestas generadas por el sistema se asemejan mucho a las que se desear√≠an o esperar√≠an, mostrando una buena comprensi√≥n del problema.

"""